{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Gradient Descent in Scikit-Learn\n",
    "\n",
    "_Authors: Kiefer Katovich (SF)_\n",
    "\n",
    "---\n",
    "\n",
    "Until now we've been using specific scikit-learn model classes such as `LinearRegression` and `LogisticRegression` to perform regression and classification. While these methods work well on smaller data sets with relatively few features, the process slows down to a crawl once you start getting into \"medium data.\" Plus, these methods take up so much memory that fitting them becomes mind-numbingly slow — especially on a laptop.\n",
    "\n",
    "Luckily, scikit-learn comes with stochastic gradient descent solvers for regression and classification:\n",
    "- `SGDRegressor`\n",
    "- `SGDClassifier`\n",
    "\n",
    "Because of its ability to minimize the loss function iteratively on smaller portions of the data, the SGD solvers avoid the intense slowdown other models suffer on large data sets.\n",
    "\n",
    "> **Note:** The gradient descent solvers are flexible and can fit a variety of different model types not covered here. We highly recommend reading their documentation in detail.\n",
    "\n",
    "---\n",
    "\n",
    "### San Francisco Assessor Data\n",
    "\n",
    "This lab uses data on housing prices in San Francisco from the S.F. Assessor's Office — the set is already cleaned up.\n",
    "\n",
    "You can see that the set has 250,000 rows. When expanding this with dummy-coded categorical columns, it can become quite large. Be careful that you don't exceed the memory on your computer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy \n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "\n",
    "import patsy\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the data.\n",
    "\n",
    "Examine the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop = pd.read_csv('../datasets/assessor_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 17)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prop.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "baths               int64\n",
       "beds                int64\n",
       "lot_depth         float64\n",
       "basement_area     float64\n",
       "front_ft          float64\n",
       "owner_pct         float64\n",
       "rooms               int64\n",
       "property_class     object\n",
       "neighborhood       object\n",
       "tax_rate          float64\n",
       "volume              int64\n",
       "sqft                int64\n",
       "stories             int64\n",
       "year_recorded       int64\n",
       "year_built          int64\n",
       "zone               object\n",
       "value             float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prop.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "baths             0\n",
       "beds              0\n",
       "lot_depth         0\n",
       "basement_area     0\n",
       "front_ft          0\n",
       "owner_pct         0\n",
       "rooms             0\n",
       "property_class    0\n",
       "neighborhood      0\n",
       "tax_rate          0\n",
       "volume            0\n",
       "sqft              0\n",
       "stories           0\n",
       "year_recorded     0\n",
       "year_built        0\n",
       "zone              0\n",
       "value             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prop.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['D', 'Z', 'DBM', 'LZ', 'TH', 'ZBM'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prop.property_class.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['01E', '10G', '10C', '02C', '02A', '04S', '08F', '10H', '03A',\n",
       "       '02F', '10B', '04T', '04J', '04M', '08E', '10F', '10E', '03G',\n",
       "       '03H', '06D', '09E', '02B', '02D', '10D', '01C', '07B', '02E',\n",
       "       '04C', '04H', '10A', '05A', '04F', '04E', '10J', '09A', '03B',\n",
       "       '07C', '01A', '01G', '09G', '02G', '05C', '05F', '03J', '05K',\n",
       "       '03E', '06A', '05D', '08C', '04G', '04R', '06B', '01D', '07A',\n",
       "       '09C', '09F', '10K', '01F', '05G', '03C', '09H', '04B', '08G',\n",
       "       '04N', '05B', '08A', '08H', '04P', '03F', '07D', '06E', '01B',\n",
       "       '05M', '05E', '06C', '04D', '05H', '08B', '05J', '09D', '04A',\n",
       "       '04K', '08D', '06F', '03D', '09B', '047', '08I'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prop.neighborhood.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Sample down the data.\n",
    "\n",
    "Even though this is already only a sample of the full assessor data set, you should sample the data down further for the sake of speed and your computer's memory.\n",
    "\n",
    "Use the `.sample()` function for Pandas DataFrames to subset this down to < 25,000 rows. \n",
    "\n",
    "Sampling down large data sets is a common procedure. Finding the optimal parameters with larger subsets of the data may change the hyperparameters and results and will get you closer to the best coefficients — but the returns become marginal at a certain point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_samp = prop.sample(n=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Regression with stochastic gradient descent.\n",
    "\n",
    "Below are x, y data predicting value (housing price) from the remaining variables. There are ~75,000 rows with 170 columns.\n",
    "\n",
    "\n",
    "The `SGDRegressor` is general and flexible and can be customized with a variety of keyword arguments.\n",
    "\n",
    "**Arguments**\n",
    "- `loss`: `['squared_loss','huber', ...]`\n",
    "    - The `'squared_loss'` corresponds to solving a regression with the least squares loss. This is what you'll probably use, but there are other options. Huber loss is a \"robust\" regression loss.\n",
    "- `penalty`: `['none','l1','l2','elasticnet']`\n",
    "    - This defines the penalty on the regression you'd like to solve. The l1 and l2 are the lasso and ridge, while the elastic net is the combination of both.\n",
    "- `alpha`\n",
    "    - The regularization strength to be used with a chosen penalty. It's the same as in the lasso and ridge.\n",
    "- `l1_ratio`\n",
    "    - The mix of the lasso and ridge penalties when elastic net is chosen as the penalty.\n",
    "- `n_iter`\n",
    "    - The number of training epochs over the data. This is the number of passes that the gradient descent algorithm will make over the data to iteratively fit the weights (defaults to five).\n",
    "\n",
    "`SGDRegressor` is most often used in tandem with grid searching to find the optimal parameters for certain models. \n",
    "\n",
    "**It's up to you how you want to define the model. You should:**\n",
    "\n",
    "1) Choose a target to estimate (this should be continuous).\n",
    "    - Select predictors to use.\n",
    "    - Standardize your predictor matrix.\n",
    "    - Build a stochastic gradient descent solver to fit your model. You'll likely want to perform some kind of grid search to find the optimal parameters for your model.\n",
    "    - Describe the model selected through grid search and compare the performance to the baseline.\n",
    "    - Examine and interpret the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value ~ baths + beds + lot_depth + basement_area + front_ft + owner_pct + rooms + property_class + neighborhood + tax_rate + volume + sqft + stories + year_recorded + year_built + zone -1\n",
      "(25000L,) (25000, 165)\n"
     ]
    }
   ],
   "source": [
    "f = 'value ~ '+' + '.join([c for c in prop_samp.columns if not c == 'value'])+' -1'\n",
    "print f\n",
    "y, X = patsy.dmatrices(f, data=prop_samp, return_type='dataframe')\n",
    "y = y.values.ravel()\n",
    "\n",
    "print y.shape, X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am predicting the value of the house from the other variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor, SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000L, 165L)\n"
     ]
    }
   ],
   "source": [
    "# Standardize the predictors\n",
    "ss = StandardScaler()\n",
    "Xs = ss.fit_transform(X)\n",
    "print Xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the gridsearch parameters:\n",
    "sgd_params = {\n",
    "    'loss':['squared_loss','huber'],\n",
    "    'penalty':['l1','l2'],\n",
    "    'alpha':np.logspace(-5,1,25)\n",
    "}\n",
    "\n",
    "sgd_reg = SGDRegressor()\n",
    "sgd_reg_gs = GridSearchCV(sgd_reg, sgd_params, cv=5, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Samson\\AppData\\Local\\Continuum\\anaconda2\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDRegressor'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# SGD is pretty fast compared to other sklearn solvers - but can still\n",
    "# take a good long while depending on the gridsearch and the size of\n",
    "# the dataset.\n",
    "sgd_reg_gs.fit(Xs, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print sgd_reg_gs.best_params_\n",
    "print sgd_reg_gs.best_score_\n",
    "sgd_reg = sgd_reg_gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that the gradient descent got a .22 R2 using the ridge and squared loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_coefs = pd.DataFrame({'coef':sgd_reg.coef_,\n",
    "                            'mag':np.abs(sgd_reg.coef_),\n",
    "                            'pred':X.columns})\n",
    "value_coefs.sort_values('mag', ascending=False, inplace=True)\n",
    "value_coefs.iloc[0:10, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Classification with stochastic gradient descent.\n",
    "\n",
    "The `SGDClassifier` is very similar to the `SGDRegressor`. The main difference is that the loss functions are changed to regression loss functions.\n",
    "\n",
    "**Arguments**\n",
    "- `loss`: `['log', ...]`\n",
    "    - The `'log'` loss corresponds to solving a logistic regression classifier. This is what you'll probably use, but there are other options.\n",
    "- `penalty`: `['none','l1','l2','elasticnet']`\n",
    "    - This defines the penalty on the regression you'd like to solve. The l1 and l2 are the lasso and ridge, while the elastic net is the combination of both.\n",
    "- `alpha`\n",
    "    - The regularization strength to be used with a chosen penalty. It's the same as in the lasso and ridge.\n",
    "- `l1_ratio`\n",
    "    - The mix of the lasso and ridge penalties when elastic net is chosen as the penalty.\n",
    "- `n_iter`\n",
    "    - The number of training epochs over the data. This is the number of passes that the gradient descent algorithm will make over the data to iteratively fit the weights (defaults to five).\n",
    "\n",
    "Like `SGDRegressor`, `SGDClassifier` is most often used in tandem with grid searching to find the optimal parameters for certain models. \n",
    "\n",
    "**It's up to you how you want to define the model. You should:**\n",
    "\n",
    "1) Choose a target to classify (you may need to engineer one from existing variables).\n",
    "    - Calculate the baseline accuracy.\n",
    "    - Select predictors to use.\n",
    "    - Standardize your predictor matrix.\n",
    "    - Build a stochastic gradient descent solver to fit your model. You'll likely want to perform some kind of grid search to find the optimal parameters for your model.\n",
    "    - Describe the model selected through grid search and compare the performance to baseline.\n",
    "    - Examine and interpret the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_samp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_samp.year_built.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see if we can predict if a house was built past 1980\n",
    "prop_samp['built_past1980'] = prop_samp.year_built.map(lambda x: 1 if x >= 1980 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the target and calculate the baseline:\n",
    "y = prop_samp.built_past1980.values\n",
    "print 1. - np.mean(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = '''\n",
    "~ baths + beds + lot_depth + basement_area + front_ft + owner_pct +\n",
    "rooms + property_class + neighborhood + tax_rate + volume + sqft + stories +\n",
    "zone + value -1\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = patsy.dmatrix(f, data=prop_samp, return_type='dataframe')\n",
    "\n",
    "Xs = ss.fit_transform(X)\n",
    "print y.shape, Xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_cls_params = {\n",
    "    'loss':['log'],\n",
    "    'penalty':['l1','l2'],\n",
    "    'alpha':np.logspace(-5,2,50)\n",
    "}\n",
    "\n",
    "sgd_cls = SGDClassifier()\n",
    "sgd_cls_gs = GridSearchCV(sgd_cls, sgd_cls_params, cv=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_cls_gs.fit(Xs, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print sgd_cls_gs.best_params_\n",
    "print sgd_cls_gs.best_score_\n",
    "sgd_cls = sgd_cls_gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
