{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Visualizing the Confusion Matrix, ROC Curve, and Precision-Recall Curve\n",
    "\n",
    "_Authors: Kiefer Katovich (SF)_\n",
    "\n",
    "---\n",
    "\n",
    "The interactive visualization below allows you to observe how the confusion matrix, ROC curve, and precision-recall curve interact with one another. \n",
    "\n",
    "The model is a logistic regression fit on the cancer versus healthy data. The raw data points are shown, along with the prediction curve (right panel). \n",
    "\n",
    "You can change the threshold and observe how it affects the confusion matrix, as well as where that threshold is on the corresponding ROC or precision-recall curve (left panel). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "from ipywidgets import *\n",
    "from IPython.display import display\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**The visualization allows you to change three variables:**\n",
    "- **Spread**: The dispersion of the data (impacts the signal and how well the classifier can discriminate between the points).\n",
    "- **Threshold**: The decision threshold for labeling 1 versus 0.\n",
    "- **Cancer %**: The number of data points that are cancerous versus healthy. This helps show the effect of class imbalance on classifier performance and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Samson\\AppData\\Local\\Continuum\\anaconda2\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c7554f56d8d4cd8a31b9ae63c4b2698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>interactive</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "interactive(children=(IntSlider(value=75, description=u'spread:'), FloatSlider(value=0.5, description=u'threshold:', max=0.99, min=0.01, step=0.01), FloatSlider(value=0.5, description=u'cancer %:', max=0.95, min=0.05, step=0.05), Checkbox(value=False, description=u'PR curve:'), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import imp\n",
    "plotter = imp.load_source('plotter', './plotting-code/roc_plotter.py')\n",
    "from plotter import ROCLogisticPlotter\n",
    "\n",
    "roc_plotter = ROCLogisticPlotter()\n",
    "roc_plotter.preconstruct_data()\n",
    "roc_plotter.roc_interact()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Relevant Classification Metrics\n",
    "\n",
    "This reference table describes some of the important metrics displayed in the visual below.\n",
    "\n",
    "|   |   |\n",
    "|---|---|\n",
    "|**TPR/RECALL**    | The true positive rate, also known as the **sensitivity** or **recall**, is the ability of the classifier to correctly identify a class. <br><br>`Recall = True Positives / (True Positives + False Negatives)`<br><br>A recall of 1 indicates that the classifier correctly predicted all observations of the class. A recall of 0 means the classifier predicted all observations of the current class incorrectly.|\n",
    "|**FPR** | The false positive rate is the percent of times the model incorrectly predicts 1 when the class is actually 0. This is the x-axis on the ROC curve.<br><br> `fpr = fp / (tn + fp)`<br><br>|\n",
    "|**PRECISION** | Precision is the ability of the classifier to avoid incorrectly labeling a class as a member of another class. <br><br> `Precision = True Positives / (True Positives + False Positives)`<br><br>_A precision score of 1 indicates that the classifier never mistakenly classified the current class as another class. A precision score of 0 would mean that the classifier misclassified every instance of the current class. |\n",
    "|**RECALL**    | Recall is the ability of the classifier to correctly identify the current class. <br><br>`Recall = True Positives / (True Positives + False Negatives)`<br><br>A recall of 1 indicates that the classifier correctly predicted all observations of the class. A recall of 0 means the classifier predicted all observations of the current class incorrectly.|\n",
    "|**AUC** | The area under the curve can refer to either the ROC curve or the precision-recall curve. In the case of the ROC curve, an area of 0.50 is the baseline, meaning this is the area under the curve when the classifier would be predicting at chance. An AUC of 1.0 is a perfect model, where the classifier never makes a mistake. <br><br>|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.66      0.62      0.64        50\n",
      "        1.0       0.64      0.68      0.66        50\n",
      "\n",
      "avg / total       0.65      0.65      0.65       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print classification_report(roc_plotter.currents['y_true'], roc_plotter.currents['y_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "widgets": {
   "state": {
    "7fe21a4d87034349995eb363e48cae76": {
     "views": [
      {
       "cell_index": 3
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
