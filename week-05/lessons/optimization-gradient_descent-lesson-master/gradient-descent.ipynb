{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Gradient Descent: Iteratively Minimizing Loss Functions \n",
    "\n",
    "_Authors: Kiefer Katovich (SF)_\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "- Review derivatives and partial derivatives.\n",
    "- Review the least squared loss function for regression.\n",
    "- Understand how gradient descent minimizes the loss function.\n",
    "- Manually code the gradient descent algorithm for simple linear regression from scratch.\n",
    "- Learn how the gradient descent code changes for multiple linear regression.\n",
    "- Visualize gradient descent optimizing the coefficients of a regression.\n",
    "- Understand the pitfalls of gradient descent and observe when it can fail.\n",
    "- Understand how stochastic gradient descent is different and what its benefits are.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Guide\n",
    "- [Introduction to Gradient Descent](#intro)\n",
    "- [Review of Derivatives](#derivatives)\n",
    "- [Review of the Least Squares Loss Function for Regression](#lsq-loss)\n",
    "    - [Exercise 1: Code the Loss Function](#exercise-1)\n",
    "- [Partial Derivatives of the Loss Function](#partial-derivatives)\n",
    "    - [Partial Derivative With Respect to $\\beta_0$](#beta0)\n",
    "    - [Partial Derivative With Respect to $\\beta_1$](#beta1)\n",
    "    - [Exercise 2: Code the Partial Derivative Functions](#exercise-2)\n",
    "    - [Iterating Toward the Minimum](#iterating)\n",
    "- [Coding the Gradient Descent Algorithm for SLR](#code-descent)\n",
    "    - [Exercise 3: Code the Beta Coefficient Update](#exercise-3)\n",
    "    - [Exercise 4: Code the Gradient Descent Iterator](#exercise-4)\n",
    "- [Exercise 5: Test the Gradient Descent Algorithm on the Simple Housing Data](#exercise-5)\n",
    "- [Linear Algebra MLR Generalization of Gradient Descent](#linalg)\n",
    "- [Interactive Visualization of Gradient Descent](#interactive)\n",
    "- [Gradient Descent can Fail: A Toy Example](#fail)\n",
    "- [Stochastic Gradient Descent](#stochastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "from ipywidgets import *\n",
    "from IPython.display import display\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "\n",
    "## Introduction to Gradient Descent\n",
    "\n",
    "---\n",
    "\n",
    "Gradient descent is an algorithm used to minimize functions, such as the least squares loss, in regression. It's popular in machine learning and statistics.\n",
    "\n",
    "The gradient descent algorithm uses the derivative of the loss function to move in the direction where the loss function is descending."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='derivatives'></a>\n",
    "## Derivatives\n",
    "---\n",
    "\n",
    "The derivative of a function quantifies the **rate of change** of the the function with respect to another quantity. \n",
    "\n",
    "Imagine the derivative as a tangent line on the edge of another function. For example, in the image below, if the black curve was the velocity of a car, the red tangent would represent the derivative of velocity at that point (the acceleration of the car).\n",
    "\n",
    "![derivative](https://camo.githubusercontent.com/2f70b084174b825e3ad88564301f9aaf46997fd3/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f302f30662f54616e67656e745f746f5f615f63757276652e737667)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The value of the derivative of a function indicates whether the function is **increasing or decreasing** at that point/input value. \n",
    "\n",
    "* If the function is not changing (the tangent line is flat), **the derivative is 0**.\n",
    "* If the function is increasing (the tangent slope is positive), **the derivative is positive**.\n",
    "* If the function is decreasing (the tangent slope is negative), **the derivative is negative**.\n",
    "\n",
    "**In the case of convex loss functions such as the least squares loss, the minimum is the point where the derivative is 0.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='lsq-loss'></a>\n",
    "\n",
    "## Review: The Least Squares Loss Function for Regression\n",
    "\n",
    "---\n",
    "\n",
    "Recall the least squares loss function:\n",
    "\n",
    "### $$\\frac{1}{N}\\sum_{i=1}^N{\\left(y_i - \\hat{y}_i\\right)^2}$$\n",
    "\n",
    "As well as the formula for a linear regression with a single predictor variable:\n",
    "\n",
    "### $$y = \\beta_0 + \\beta_1x_1$$\n",
    "\n",
    "We can redefine the loss function by inserting the regression formula:\n",
    "\n",
    "### $$\\frac{1}{N}\\sum_{i=1}^N{\\left(y_i - (\\beta_0 + \\beta_1x_i)\\right)^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<a id='exercise-1'></a>\n",
    "### Exercise 1: Code the Loss Function\n",
    "\n",
    "Write a Python function for the least squares loss of this simple linear regression in terms of:\n",
    "- `y`: Vector of y values.\n",
    "- `beta_0`: Intercept coefficient.\n",
    "- `beta_1`: Coefficient for predictor `x`.\n",
    "- `x`: Vector of predictor values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(y, X, beta_0, beta_1):\n",
    "    loss = np.sum((y-(beta_0+beta_1*X))**2)/len(y)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='partial-derivatives'></a>\n",
    "## Partial Derivatives of the Loss Function\n",
    "\n",
    "---\n",
    "\n",
    "We're going to calculate the two partial derivatives of the loss function. Partial derivatives are derivatives with respect to one variable while the other variables remain constant. Our partial derivatives will be:\n",
    "\n",
    "* The derivative of the loss function with respect to `beta0` (the intercept).\n",
    "* The derivative of the loss function with respect to `beta1` (the slope/coefficient for x1).\n",
    "\n",
    "The loss function is defined by these two parameters. In other words, the value of the loss function depends on the changes in `beta0` and `beta1`. \n",
    "\n",
    "What about `x` and `y`? Those variables affect the calculation of the loss, but we're not able to change them to adjust the error. We can only adjust the parameters of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='beta0'></a>\n",
    "### The partial derivative with respect to $\\beta_0$:\n",
    "\n",
    "### $$\\frac{\\delta}{\\delta\\beta_0} = \\frac{2}{N}\\sum_{i=1}^N{-\\left(y_i - (\\beta_0 + \\beta_1x_1)\\right)}$$\n",
    "\n",
    "<a id='beta1'></a>\n",
    "### The partial derivative with respect to $\\beta_1$:\n",
    "\n",
    "### $$\\frac{\\delta}{\\delta\\beta_1} = \\frac{2}{N}\\sum_{i=1}^N{-x_i\\left(y_i - (\\beta_0 + \\beta_1x_1)\\right)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='exercise-2'></a>\n",
    "### Exercise 2: Code the Partial Derivative Functions\n",
    "\n",
    "The functions should return the gradients (partial derivatives) of `beta0` and `beta1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdev_beta_0(y, X, beta_0, beta_1):\n",
    "    updated_beta_0 = np.sum(-(y-(beta_0+beta_1*X)))*(2/float(len(y)))\n",
    "    return beta_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdev_beta_1(y, X, beta_0, beta_1):\n",
    "    updated_beta_1 = np.sum(-X*(y-(beta_0+beta_1*X)))*(2/float(len(y)))\n",
    "    return updated_beta_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "\n",
    "<a id='iterating'></a>\n",
    "### Iterating Toward the Minimum\n",
    "\n",
    "So what are we going to do with these partial derivatives?\n",
    "\n",
    "Recall that a positive derivative indicates an increasing function and a negative derivative indicates a decreasing function. \n",
    "\n",
    "If we were to subtract a tiny fraction of the partial derivative of $\\beta_1$ from $\\beta_1$, and subtract a tiny fraction of the partial derivative of $\\beta_0$ from $\\beta_0$, we'll adjust the beta coefficients so that the value of the loss function shrinks.\n",
    "\n",
    "We can repeat this incremental process until we reach the minimum of the function (or at least close to the minimum).\n",
    "\n",
    "This is called gradient descent because **we're iteratively moving down the gradient of the error function to its minimum.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](https://upload.wikimedia.org/wikipedia/commons/7/79/Gradient_descent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='code-descent'></a>\n",
    "## Coding the Gradient Descent Algorithm for SLR\n",
    "\n",
    "---\n",
    "\n",
    "We'll now code the gradient descent algorithm to find the optimal intercept and slope for a simple linear regression using the least squares loss function.\n",
    "\n",
    "We've already coded a few of the components into functions:\n",
    "- **The mean squared error**.\n",
    "- **The beta_0 gradient (partial derivative)**.\n",
    "- **The beta_1 gradient (partial derivative)**.\n",
    "\n",
    "Now we're going to code two more functions:\n",
    "- **The beta coefficient update function**.\n",
    "- **The gradient descent iteration function**.\n",
    "\n",
    "<a id='exercise-3'></a>\n",
    "### Exercise 3: Code the Beta Coefficient Update Function\n",
    "\n",
    "The beta coefficient update function iterates through every observed `y` and predictor `x`. It will calculate the changes to the current `beta0` and `beta1` values to move the loss function (mean squared error) closer to its minimum.\n",
    "\n",
    "The update function will take these arguments:\n",
    "- `y`: The vector of observed target values.\n",
    "- `x`: The vector of predictor values.\n",
    "- `beta0`: The current value of the intercept.\n",
    "- `beta1`: The current value of the coefficient for `x`.\n",
    "- `step_size`: A step size by which to multiply the gradients.\n",
    "\n",
    "The **step size** controls how much the gradient update should modify the coefficients. It's good to take small steps toward the minimum so the algorithm doesn't overshoot and spin out of control.\n",
    "\n",
    "**Your function should return the new values of `beta0` and `beta1`.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(y, X, beta_0, beta_1):\n",
    "    loss = np.sum((y-(beta_0+beta_1*X))**2)/len(y)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdev_beta_0(y, X, beta_0, beta_1):\n",
    "    updated_beta_0 = np.sum(-(y-(beta_0+beta_1*X)))*(2/float(len(y)))\n",
    "    return beta_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdev_beta_1(y, X, beta_0, beta_1):\n",
    "    updated_beta_1 = np.sum(-X*(y-(beta_0+beta_1*X)))*(2/float(len(y)))\n",
    "    return updated_beta_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_func(y, X, beta_0, beta_1, step_size, iter=10):\n",
    "    beta_0_ls = []\n",
    "    beta_1_ls = []\n",
    "    loss_ls = []\n",
    "    for i in range(iter):\n",
    "        beta_0 -= step_size*pdev_beta_0(y, X, beta_0, beta_1)\n",
    "        beta_1 -= step_size*pdev_beta_1(y, X, beta_0, beta_1)\n",
    "        loss = loss_func(y, X, beta_0, beta_1)\n",
    "        beta_0_ls.append(beta_0)\n",
    "        beta_1_ls.append(beta_1)\n",
    "        loss_ls.append(loss) \n",
    "        print i, beta_0, beta_1, loss\n",
    "    return beta_0_ls, beta_1_ls, loss_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdev_beta_0(y, X, 0.5, 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(np.random.normal(loc=5,scale=10,size=10))\n",
    "X = np.array(np.random.normal(loc=2,scale=2,size=10))\n",
    "beta_0 = 0.2\n",
    "beta_1 = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.1998 0.18504038272052503 110.56970138606582\n",
      "1 0.1996002 0.17041785315753422 110.35840200284011\n",
      "2 0.1994005998 0.15612484053784675 110.15654064157943\n",
      "3 0.1992011992 0.1421539440981764 109.96369774922701\n",
      "4 0.199001998001 0.12849792926740017 109.77947241338761\n",
      "5 0.198802996003 0.11514972393455819 109.60348153432338\n",
      "6 0.198604193007 0.10210241480065856 109.43535903372349\n",
      "7 0.198405588814 0.0893492438124061 109.27475509861672\n",
      "8 0.198207183225 0.07688360467601502 109.12133545886381\n",
      "9 0.198008976042 0.0646990394493071 108.97478069674003\n",
      "10 0.197810967066 0.05278923521033754 108.83478558718181\n",
      "11 0.197613156099 0.041148020800829785 108.7010584673362\n",
      "12 0.197415542943 0.02976936364273967 108.57332063411134\n",
      "13 0.1972181274 0.018647366626306533 108.45130576848484\n",
      "14 0.197020909272 0.007776265067986005 108.33475938538099\n",
      "15 0.196823888363 -0.0028495762633047143 108.2234383079824\n",
      "16 0.196627064475 -0.013235666053163219 108.11711016538952\n",
      "17 0.19643043741 -0.023387389283399312 108.0155529125922\n",
      "18 0.196234006973 -0.033310010009919566 107.9185543717621\n",
      "19 0.196037772966 -0.043008674078231834 107.82591179391848\n"
     ]
    }
   ],
   "source": [
    "update_func(y, X, beta_0, beta_1, 0.001, iter=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='exercise-4'></a>\n",
    "### Exercise 4: Code the Gradient Descent Iterator\n",
    "\n",
    "Finally, we can put it all together by coding a function that will iterate down the gradient of the loss function toward the minimum. At each step, the function will call the gradient update function.\n",
    "\n",
    "We should keep track of the beta coefficients and the mean squared error in lists as the algorithm progresses.\n",
    "\n",
    "**The arguments to the function will be:**\n",
    "- `x`: The vector of predictors.\n",
    "- `y`: The vector of observed target values.\n",
    "- `beta0`: An initial value for the intercept.\n",
    "- `beta1`: An initial value for the slope.\n",
    "- `step_size`: A step size for the gradient update.\n",
    "- `iterations`: How many times the gradient update function should be called before stopping.\n",
    "\n",
    "> **Note:** Make sure your step size is quite small (`0.0000001`, for example) or the MSE will increase.\n",
    "\n",
    "At each iteration, keep track of the current `beta0`, `beta1`, and the mean squared error.\n",
    "\n",
    "**The function should return the lists of `beta0`, `beta1`, and `mse` values for the iterations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='exercise-5'></a>\n",
    "\n",
    "## Exercise 5: Test Your Gradient Descent Code on the Simple Housing Data Set\n",
    "\n",
    "---\n",
    "\n",
    "Load in the simple housing data. Set up two variables:\n",
    "- `y`: The price of the house, divided by 1,000.\n",
    "- `x`: The square feet (sq ft) of the house.\n",
    "\n",
    "Initialize starting values for `beta0` and `beta1`. Then, run your gradient descent iterator, returning the array of MSEs and coefficients at each step. \n",
    "\n",
    "Finally, plot the trajectory of the MSEs, beta0s, and beta1s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "house = pd.read_csv('./datasets/housing-data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sqft</th>\n",
       "      <th>bdrms</th>\n",
       "      <th>age</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>47.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>47.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2000.680851</td>\n",
       "      <td>3.170213</td>\n",
       "      <td>42.744681</td>\n",
       "      <td>340412.659574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>794.702354</td>\n",
       "      <td>0.760982</td>\n",
       "      <td>22.873440</td>\n",
       "      <td>125039.899586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>852.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>169900.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1432.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>24.500000</td>\n",
       "      <td>249900.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1888.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>299900.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2269.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>61.500000</td>\n",
       "      <td>384450.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4478.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>699900.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              sqft      bdrms        age          price\n",
       "count    47.000000  47.000000  47.000000      47.000000\n",
       "mean   2000.680851   3.170213  42.744681  340412.659574\n",
       "std     794.702354   0.760982  22.873440  125039.899586\n",
       "min     852.000000   1.000000   5.000000  169900.000000\n",
       "25%    1432.000000   3.000000  24.500000  249900.000000\n",
       "50%    1888.000000   3.000000  44.000000  299900.000000\n",
       "75%    2269.000000   4.000000  61.500000  384450.000000\n",
       "max    4478.000000   5.000000  79.000000  699900.000000"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "house.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = house['price']/1000\n",
    "X = house['sqft']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_func(y, X, beta_0, beta_1, step_size, iter=10):\n",
    "    beta_0_ls = []\n",
    "    beta_1_ls = []\n",
    "    loss_ls = []\n",
    "    for i in range(iter):\n",
    "        beta_0 -= step_size*pdev_beta_0(y, X, beta_0, beta_1)\n",
    "        beta_1 -= step_size*pdev_beta_1(y, X, beta_0, beta_1)\n",
    "        loss = loss_func(y, X, beta_0, beta_1)\n",
    "        beta_0_ls.append(beta_0)\n",
    "        beta_1_ls.append(beta_1)\n",
    "        loss_ls.append(loss) \n",
    "        print i, beta_0, beta_1, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.753459180576844"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -3.47589256807 -3.469399898656982 61104319.90484157\n",
      "1 -3.47588909218 30.135999377721703 4150199928.074133\n",
      "2 -3.47588561629 -246.82884192307398 281902989558.10596\n",
      "3 -3.4758821404 2035.825879170353 19148327140475.527\n",
      "4 -3.47587866452 -16777.07669465082 1300654664923697.2\n",
      "5 -3.47587518864 138272.7869493164 8.834727678253398e+16\n",
      "6 -3.47587171277 -1139598.0850057853 6.001009741796039e+18\n",
      "7 -3.47586823689 9392200.699035313 4.076200108552921e+20\n",
      "8 -3.47586476103 -77407481.53945838 2.768768597265114e+22\n",
      "9 -3.47586128516 637967450.2452312 1.8806926404609055e+24\n",
      "10 -3.4758578093 -5257921564.876009 1.27746493924322e+26\n",
      "11 -3.47585433344 43334090447.49024 8.677210916270439e+27\n",
      "12 -3.47585085759 -357145570101.4442 5.8940161074046866e+29\n",
      "13 -3.47584738174 2943478377566.6084 4.0035244284781366e+31\n",
      "14 -3.47584390589 -24259197605996.098 2.7194034691023123e+33\n",
      "15 -3.47584043005 199936467334722.6 1.8471612600043035e+35\n",
      "16 -3.4758369542 -1647811754516061.2 1.2546886694922865e+37\n",
      "17 -3.47583347837 1.35807319921059e+16 8.522502563466802e+38\n",
      "18 -3.47583000253 -1.1192800447984115e+17 5.788930091613033e+40\n",
      "19 -3.4758265267 9.224744435072754e+17 3.9321445028642985e+42\n"
     ]
    }
   ],
   "source": [
    "update_func(y, X, np.random.randn(), np.random.randn(), 0.000001, iter=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0xe033588>]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xdf777b8>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 244,
       "width": 419
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='linalg'></a>\n",
    "\n",
    "## Linear Algebra MLR Generalization of the Gradient Descent Code\n",
    "\n",
    "---\n",
    "\n",
    "Now that we've coded the gradient descent for a simple linear regression, we can generalize this code to work for a matrix of predictors instead of just one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### New Mean Squared Error Loss Function\n",
    "\n",
    "This function calculates the mean of the squared errors using a [dot product](http://mathworld.wolfram.com/DotProduct.html) between the `X` predictor matrix and the `beta_array`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def mean_squared_error(X, y, beta_array):\n",
    "    y_hat = np.dot(X, beta_array)\n",
    "    mean_sq_err = np.mean((y_true - y_hat)**2)\n",
    "    return mean_sq_err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### New $\\beta$ Update Function\n",
    "\n",
    "This will update the value of $\\beta$ array. We still use the partial derivative formulas above, with some linear algebra tweaks to make it work with an arbitrary $X$ predictor matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beta_update_function(X, y, beta_array, step_size):\n",
    "    # Create a transposed version of the X predictor array.\n",
    "    Xt = X.T\n",
    "    \n",
    "    # Predictions are the dot product of the X matrix and beta matrix.\n",
    "    y_hat = np.dot(X, beta_array)\n",
    "    \n",
    "    # Residuals are the true y minus the predicted y.\n",
    "    residuals = y_hat - y\n",
    "    \n",
    "    # Calculate the gradient from the partial derivatives.\n",
    "    # This actually does both at the same time.\n",
    "    # How? Because the beta0 column is all 1s, the \n",
    "    # dot product turns out to be the same as the beta0\n",
    "    # partial derivative function.\n",
    "    gradient = np.dot(Xt, residuals) / (X.shape[0]/2.)\n",
    "    \n",
    "    # Update the betas with the gradient.\n",
    "    beta_array = beta_array - (step_size * gradient)\n",
    "    \n",
    "    return beta_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### New Gradient Descent Iteration Update Function\n",
    "\n",
    "This is the function that wraps the gradient update with some number of iterations. It's the same except it takes an array of beta coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def run_gradient_descent(X, y, initial_beta_array, step_size, iterations=500):\n",
    "    \n",
    "    beta_array = initial_beta_array\n",
    "    \n",
    "    # Set up the MSE tracker.\n",
    "    mses = []\n",
    "    mses.append(mean_squared_error(X, y, beta_array))\n",
    "    \n",
    "    # Track the betas over the iterations.\n",
    "    beta_arrays = []\n",
    "\n",
    "    # Update the betas via gradient descent.\n",
    "    for i in range(iterations):\n",
    "        beta_array = beta_update_function(X, y, beta_array, step_size)\n",
    "        mses.append(mean_squared_error(X, y, beta_array))\n",
    "        beta_arrays.append(beta_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='interactive'></a>\n",
    "\n",
    "## Interactive Visualization of Gradient Descent\n",
    "\n",
    "This interactive visualization lets you watch gradient descent solve the optimal coefficient values.\n",
    "\n",
    "> **Note:** Because this is a simple implementation, there's no automatic adjustment of step size, so setting this value can be finicky and you'll have to play around with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34c1e9c865044041b06f7fc38c1e4c51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>interactive</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description=u'Iteration:', max=5000), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import imp\n",
    "plotter = imp.load_source('plotter', './code/gradient_descent.py')\n",
    "from plotter import GradientDescentPlotter\n",
    "\n",
    "gd_plotter = GradientDescentPlotter(step_size=0.001)\n",
    "gd_plotter.run_gradient_descent(iterations=5000)\n",
    "gd_plotter.gradient_interact()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='fail'></a>\n",
    "\n",
    "## Gradient Descent Can Fail: A Toy Example\n",
    "\n",
    "---\n",
    "\n",
    "One of the most delicate things about gradient descent is the step size (also known as learning rate). If it's not tuned properly, the algorithm may never converge and may explode into extreme values.\n",
    "\n",
    "But that's not the only pitfall with gradient descent. It can also get stuck in \"local minima\" and only works where there's a gradient to follow. \n",
    "\n",
    "Here's a toy example of a function where gradient descent will fail:\n",
    "\n",
    "$$f(x, y) = \\begin{cases}\n",
    "2 x^2 & \\quad \\text{if $x \\leq 1$}\\\\\n",
    "2  & \\quad \\text{else}\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# def func(x):\n",
    "#     if x <= 1:\n",
    "#         return 2 * x * x\n",
    "#     return 2\n",
    "\n",
    "# def gradient(x):\n",
    "#     if x <= 1:\n",
    "#         return 4 * x\n",
    "#     return 0\n",
    "\n",
    "# def gradient_descent(x, l=0.1):\n",
    "#     vector = np.array(x)\n",
    "#     return vector - l * np.array(gradient(x))\n",
    "\n",
    "\n",
    "# def iterate(x0, n=10):\n",
    "#     xs = [x0]\n",
    "#     ys = [func(x0)]\n",
    "#     for i in range(n):\n",
    "#         x = gradient_descent(xs[-1], l=0.1)\n",
    "#         xs.append(x)\n",
    "#         ys.append(func(x))\n",
    "#     return xs, ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Run the gradient descent algorithm starting at `x = -1.5` first, then try it at `x = 2`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# xs = np.arange(-2, 3, 0.1)\n",
    "# ys = map(func, xs)\n",
    "\n",
    "# plt.figure(figsize=(10,8))\n",
    "# plt.plot(xs, ys, alpha=0.5, ls='dashed')\n",
    "\n",
    "# # Start gradient descent at x = -1.5.\n",
    "# xs2, ys2 = iterate(-1.5, n=10)\n",
    "# plt.scatter(xs2, ys2, c='r', s=100)\n",
    "\n",
    "# # Start gradient descent at x = 2.\n",
    "# xs2, ys2 = iterate(2, n=10)\n",
    "# plt.scatter(xs2, ys2, c='y', s=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='stochastic'></a>\n",
    "## Stochastic Gradient Descent\n",
    "\n",
    "---\n",
    "\n",
    "What is the difference between gradient descent and *stochastic* gradient descent? It's actually very small, but it has big implications.\n",
    "\n",
    "Instead of **all** the samples updating the gradient at a time, **only one** sample updates the gradient within each overall iteration (iterating over all the observations, although this can change based on specification).\n",
    "\n",
    "**Stochastic gradient descent has some advantages over gradient descent:**\n",
    "- It solves faster, as it immediately starts to update the gradient.\n",
    "- It can handle much larger data sets, as it only needs to calculate a single row or small batch of rows of the entire data set.\n",
    "\n",
    "When using scikit-learn, there are only implementations of stochastic gradient descent solvers: `SGDRegressor` and `SGDClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
