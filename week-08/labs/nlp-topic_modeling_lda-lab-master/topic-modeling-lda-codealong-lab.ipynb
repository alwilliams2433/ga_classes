{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Guided Practice With Topic Modeling and LDA\n",
    "\n",
    "_Authors: Dave Yerrington (SF)_\n",
    "\n",
    "---\n",
    "\n",
    "> **Note: This lab is intended to be completed with guidance from the instructor.**\n",
    "\n",
    "You'll rarely need to build an unsupervised topic model like LDA from scratch. Luckily, scikit-learn comes with an LDA topic modeling functionality. \n",
    "\n",
    "Let's explore a brief walk through of LDA and topic modeling using gensim. The `gensim` package is another popular LDA module. We'll work with a small collection of documents represented as a list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Load the packages and create the small \"documents.\"\n",
    "\n",
    "You may need to install the `gensim` package with `pip` or `conda`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Samson\\AppData\\Local\\Continuum\\anaconda2\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models, matutils\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "doc_a = \"Brocolli is good to eat. My brother likes to eat good brocolli, but not my mother.\"\n",
    "doc_b = \"My mother spends a lot of time driving my brother around to baseball practice.\"\n",
    "doc_c = \"Some health experts suggest that driving may cause increased tension and blood pressure.\"\n",
    "doc_d = \"I often feel pressure to perform well at school, but my mother never seems to drive my brother to do better.\"\n",
    "doc_e = \"Health professionals say that brocolli is good for your health.\"\n",
    "\n",
    "# Compile the sample documents into a list.\n",
    "documents = [doc_a, doc_b, doc_c, doc_d, doc_e]\n",
    "df        = pd.DataFrame(documents, columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Brocolli is good to eat. My brother likes to e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My mother spends a lot of time driving my brot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Some health experts suggest that driving may c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I often feel pressure to perform well at schoo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Health professionals say that brocolli is good...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Brocolli is good to eat. My brother likes to e...\n",
       "1  My mother spends a lot of time driving my brot...\n",
       "2  Some health experts suggest that driving may c...\n",
       "3  I often feel pressure to perform well at schoo...\n",
       "4  Health professionals say that brocolli is good..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Load stop words either from NLTK or scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Use `CountVectorizer` to transform our text and take out the stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvt      =  CountVectorizer(strip_accents='unicode', stop_words=stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(cvt.fit_transform(df['text']).todense(),\n",
    "             columns=cvt.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Extract the tokens that remain after stop word removal.\n",
    "\n",
    "The `.vocabulary_` attribute of the vectorizer contains a dictionary of terms. There's also the built-in `.get_feature_names()` function, which will extract the column names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Get counts of the tokens.\n",
    "\n",
    "Convert the matrix from a vectorizer to a dense matrix, then sum by column to get the counts per term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ct = X_train.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Set up the vocabulary dictionary.\n",
    "\n",
    "First, we need to set up the vocabulary. Gensim's LDA expects our vocabulary to be formatted such that the dictionary keys are the column indices and the values are the words themselves, like this:\n",
    "\n",
    "{0: u'baseball',  \n",
    " 1: u'better',  \n",
    " 2: u'blood',  \n",
    " 3: u'brocolli',  \n",
    " 4: u'cause',  \n",
    " 5: u'drive',  \n",
    " 6: u'driving',  \n",
    " 7: u'eat',  \n",
    " 8: u'experts'}  \n",
    "\n",
    "Create this dictionary below.  \n",
    "\n",
    "HINT: vectorizer.vocabulary_.iteritems()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: u'around',\n",
       " 1: u'baseball',\n",
       " 2: u'better',\n",
       " 3: u'blood',\n",
       " 4: u'brocolli',\n",
       " 5: u'brother',\n",
       " 6: u'cause',\n",
       " 7: u'drive',\n",
       " 8: u'driving',\n",
       " 9: u'eat',\n",
       " 10: u'experts',\n",
       " 11: u'feel',\n",
       " 12: u'good',\n",
       " 13: u'health',\n",
       " 14: u'increased',\n",
       " 15: u'likes',\n",
       " 16: u'lot',\n",
       " 17: u'may',\n",
       " 18: u'mother',\n",
       " 19: u'never',\n",
       " 20: u'often',\n",
       " 21: u'perform',\n",
       " 22: u'practice',\n",
       " 23: u'pressure',\n",
       " 24: u'professionals',\n",
       " 25: u'say',\n",
       " 26: u'school',\n",
       " 27: u'seems',\n",
       " 28: u'spends',\n",
       " 29: u'suggest',\n",
       " 30: u'tension',\n",
       " 31: u'time',\n",
       " 32: u'well'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict1 = {}\n",
    "for i, e in cvt.vocabulary_.iteritems():\n",
    "    dict1[e] = i\n",
    "dict1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Create a token to ID mapping with gensim's `corpora.Dictionary`.\n",
    "\n",
    "This dictionary class is a more standard way to work with gensim models. There are a few standard steps we should take:\n",
    "\n",
    "**7.A) Count the frequency of the words.**\n",
    "\n",
    "We can easily do this with the Python `defaultdict(int)` function, which doesn't require us to have the key in the dictionary to be able to add to it:\n",
    "\n",
    "```python\n",
    "frequency = defaultdict(int)\n",
    "\n",
    "for text in documents:\n",
    "    for token in text.split():\n",
    "        frequency[token] += 1\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = defaultdict(int)\n",
    "\n",
    "for text in documents:\n",
    "    for token in text.split():\n",
    "        frequency[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'Brocolli': 1,\n",
       "             'Health': 1,\n",
       "             'I': 1,\n",
       "             'My': 2,\n",
       "             'Some': 1,\n",
       "             'a': 1,\n",
       "             'and': 1,\n",
       "             'around': 1,\n",
       "             'at': 1,\n",
       "             'baseball': 1,\n",
       "             'better.': 1,\n",
       "             'blood': 1,\n",
       "             'brocolli': 1,\n",
       "             'brocolli,': 1,\n",
       "             'brother': 3,\n",
       "             'but': 2,\n",
       "             'cause': 1,\n",
       "             'do': 1,\n",
       "             'drive': 1,\n",
       "             'driving': 2,\n",
       "             'eat': 1,\n",
       "             'eat.': 1,\n",
       "             'experts': 1,\n",
       "             'feel': 1,\n",
       "             'for': 1,\n",
       "             'good': 3,\n",
       "             'health': 1,\n",
       "             'health.': 1,\n",
       "             'increased': 1,\n",
       "             'is': 2,\n",
       "             'likes': 1,\n",
       "             'lot': 1,\n",
       "             'may': 1,\n",
       "             'mother': 2,\n",
       "             'mother.': 1,\n",
       "             'my': 4,\n",
       "             'never': 1,\n",
       "             'not': 1,\n",
       "             'of': 1,\n",
       "             'often': 1,\n",
       "             'perform': 1,\n",
       "             'practice.': 1,\n",
       "             'pressure': 1,\n",
       "             'pressure.': 1,\n",
       "             'professionals': 1,\n",
       "             'say': 1,\n",
       "             'school,': 1,\n",
       "             'seems': 1,\n",
       "             'spends': 1,\n",
       "             'suggest': 1,\n",
       "             'tension': 1,\n",
       "             'that': 2,\n",
       "             'time': 1,\n",
       "             'to': 6,\n",
       "             'well': 1,\n",
       "             'your': 1})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.B) Remove any words that appear only once or in the stop words.**\n",
    "\n",
    "Iterate through the documents and only keep the useful words and tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_fil = []\n",
    "for word, freq in frequency.iteritems():\n",
    "    if word not in stop:\n",
    "        freq_fil.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.C) Create the `corpora.Dictionary` object with the retained tokens.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict2 = {}\n",
    "for i, e in enumerate(freq_fil):\n",
    "    dict2[i] = e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.D) Use the `dictionary.doc2bow()` function to convert the texts to bag-of-words representations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct = Dictionary([freq_fil])\n",
    "X = dct.doc2bow(freq_fil)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why should we use this process?**\n",
    "\n",
    "The main advantage is that this dictionary object has quick helper functions.\n",
    "\n",
    "There are also some major performance advantages. It can take a while for tokenization to be computed, especially when the text files are quite large. You can save these post-computed dictionary items to file, then quickly load them from a disk.\n",
    "\n",
    "It's also possible to add new documents to your corpus without having to re-tokenize your entire set. This is great for online systems that can take new documents on demand.  \n",
    "\n",
    "This is a much better way to handle LDA and other gensim models as you work with larger text data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8) Set up the LDA model.\n",
    "\n",
    "We can create the gensim LDA model object like so:\n",
    "\n",
    "```python\n",
    "lda = models.LdaModel(\n",
    "    # Supply our sparse predictor matrix wrapped in a matutils.Sparse2Corpus object:\n",
    "    matutils.Sparse2Corpus(X, documents_columns=False),\n",
    "    # or, alternatively use the corpus object created with the dictionary in the previous frame!\n",
    "    # Corpus,\n",
    "    # the number of topics we want:\n",
    "    num_topics  =  3,\n",
    "    # How many passes over the vocabulary:\n",
    "    passes      =  20,\n",
    "    # The id2word vocabulary we made ourselves:\n",
    "    id2word     =  vocab\n",
    "    # or, use the gensim dictionary object!\n",
    "    # id2word     =  dictionary\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'tocsr'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-7de5253190dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m lda = models.LdaModel(\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m# Supply our sparse predictor matrix wrapped in a matutils.Sparse2Corpus object:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mmatutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparse2Corpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocuments_columns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;31m# or, alternatively use the corpus object created with the dictionary in the previous frame!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# Corpus,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Samson\\AppData\\Local\\Continuum\\anaconda2\\lib\\site-packages\\gensim\\matutils.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sparse, documents_columns)\u001b[0m\n\u001b[0;32m    561\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtocsc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 563\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtocsr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m  \u001b[1;31m# make sure shape[1]=number of docs (needed in len())\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    564\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    565\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'tocsr'"
     ]
    }
   ],
   "source": [
    "lda = models.LdaModel(\n",
    "    # Supply our sparse predictor matrix wrapped in a matutils.Sparse2Corpus object:\n",
    "    matutils.Sparse2Corpus(X, documents_columns=False),\n",
    "    # or, alternatively use the corpus object created with the dictionary in the previous frame!\n",
    "    # Corpus,\n",
    "    # the number of topics we want:\n",
    "    num_topics  =  3,\n",
    "    # How many passes over the vocabulary:\n",
    "    passes      =  20,\n",
    "    # The id2word vocabulary we made ourselves:\n",
    "    id2word     =  vocab\n",
    "    # or, use the gensim dictionary object!\n",
    "    # id2word     =  dictionary\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9) Look at the topics.\n",
    "\n",
    "The model has a `.print_topics()` function that accepts the number of topics to print and the number of words per topic. The number before the word is the probability that the word occurs in the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for topic in lda.print_topics(num_topics=3, num_words=5):\n",
    "    print(topic[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10) Get the topic scores for a document.\n",
    "\n",
    "The `.get_document_topics()` function accepts a bag-of-words representation for a document and returns the scores for each topic.  \n",
    "\n",
    "HINT: dictionary.doc2bow(texts[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11) Label and visualize the topics.\n",
    "\n",
    "Let's come up with some high-level labels. This is the subjective part of LDA. What do the word probabilities that represent topics mean? Let's make some up.\n",
    "\n",
    "Plot a heat map of the topic probabilities for each of the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12) Fit an LDA model with scikit-learn.\n",
    "\n",
    "Scikit-learn's LDA model is in the decomposition submodule:\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "```\n",
    "\n",
    "One of the greatest benefits of scikit-learn implementation is that it comes with the familiar `.fit()`, `.transform()`, and `.fit_transform()` methods.\n",
    "\n",
    "**12.A) Initialize and fit a scikit-learn LDA with `n_topics=3` on our output from the `CountVectorizer`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**12.B) Print out the topic-word distributions using the `.components_` attribute.**\n",
    "\n",
    "Each row of this matrix represents a topic, and the columns represent the words. (These are not probabilities.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**12.C) Use the `.transform()` method to convert the matrix into the topic scores.**\n",
    "\n",
    "These are the document-topic distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13) Further steps.\n",
    "\n",
    "This has been a very basic example. LDA typically doesn't perform well on small data sets. Try to see how it behaves on your own using a larger one. Keep in mind that finding the optimal number of topics can be tricky and subjective.\n",
    "\n",
    "**Generally, you should consider:**\n",
    "- How well topics are applied to the documents overall.\n",
    "- The strength of the topics overall to all documents.\n",
    "- Improving preprocessing, such as stop word removal.\n",
    "- Building a nice web interface to explore your documents (see: [LDAExplorer](https://github.com/dyerrington/LDAExplorer) and [pyLDAvis](https://github.com/bmabey/pyLDAvis/blob/master/README.rst)).\n",
    "\n",
    "These general guidelines should help you tune your hyperparameter **k** for the number of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
