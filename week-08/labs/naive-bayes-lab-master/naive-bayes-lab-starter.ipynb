{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png\" style=\"float: left; margin: 15px;\">\n",
    "\n",
    "## Naive Bayes Language Detection Lab\n",
    "\n",
    "_Author: David Yerrington (SF) _\n",
    "\n",
    "In this lab, we’ll use Naive Bayes (and other classifiers) to auto-detect the language of a given tweet. We’ll then assess the performance of our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, seaborn as sns, numpy as np, matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer \n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, ShuffleSplit\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = pd.read_csv(\"datasets/tweets_language.csv\", encoding=\"utf-8\", index_col=0)\n",
    "tweets_df.index = tweets_df.index.astype(int)    # By default, everything that is read in is a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 9431 entries, 0 to 9408\n",
      "Data columns (total 2 columns):\n",
      "LANG    9409 non-null object\n",
      "TEXT    9409 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 221.0+ KB\n"
     ]
    }
   ],
   "source": [
    "tweets_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Some of the rows above are null, so we can't use them for training.\n",
    "tweets_df = tweets_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LANG</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>en</td>\n",
       "      <td>The #Yolo bailout: Greece's ex-finance chief h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>en</td>\n",
       "      <td>Another mental Saturday night. It will be near...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>en</td>\n",
       "      <td>Sometimes you take bedtime selfies w yer hat s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>en</td>\n",
       "      <td>Currently just changed my entire outfit includ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>en</td>\n",
       "      <td>I just like listening to @SpotifyAU's top 100 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  LANG                                               TEXT\n",
       "0   en  The #Yolo bailout: Greece's ex-finance chief h...\n",
       "1   en  Another mental Saturday night. It will be near...\n",
       "2   en  Sometimes you take bedtime selfies w yer hat s...\n",
       "3   en  Currently just changed my entire outfit includ...\n",
       "4   en  I just like listening to @SpotifyAU's top 100 ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9409, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en     1400\n",
       "es     1179\n",
       "fr     1155\n",
       "de     1112\n",
       "pt     1030\n",
       "it      896\n",
       "ru      745\n",
       "sv      675\n",
       "und     616\n",
       "zh      405\n",
       "uk      192\n",
       "tl        4\n",
       "Name: LANG, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df['LANG'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Data exploration.\n",
    "\n",
    "#### 1.A) Explore a list of tweet words that occur more than 50 times.\n",
    "Plot a histogram that might be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the CountVectorizer to count words for us.\n",
    "cvt      =  CountVectorizer(strip_accents='unicode', ngram_range=(1,1))\n",
    "X_all    =  cvt.fit_transform(tweets_df['TEXT'])\n",
    "\n",
    "# Complete the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = cvt.fit(tweets_df['TEXT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-504dff0710d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcvt\u001b[0m      \u001b[1;33m=\u001b[0m  \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstrip_accents\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'unicode'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mngram_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m X_train = pd.DataFrame(cvt.fit_transform(df['TEXT']).todense(),\n\u001b[0m\u001b[0;32m      3\u001b[0m              columns=cvt.get_feature_names())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "cvt      =  CountVectorizer(strip_accents='unicode', ngram_range=(1,1))\n",
    "X_train = pd.DataFrame(cvt.fit_transform(df['TEXT']).todense(),\n",
    "             columns=cvt.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = X_train.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(word_count, 30, range=[1, 100], facecolor='gray', align='mid');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.B) Investigate the `counts` histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.log(word_count), 30, range=[1, 5], facecolor='gray', align='mid');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.C) Try it again with stop word removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the CountVectorizer to count words for us.\n",
    "cvt      =  CountVectorizer(strip_accents='unicode', stop_words='english')\n",
    "X_all    =  cvt.fit_transform(tweets_df['TEXT'])\n",
    "\n",
    "# Complete the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_all.todense(),\n",
    "             columns=cvt.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ct = X_train.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(word_ct, 30, range=[1, 100], facecolor='gray', align='mid');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ct.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.D) Explore n-grams between two and four.\n",
    "Display the top 75 n-grams with frequencies. Look at each class to see their similarities and differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Look up the appropriate parameters.\n",
    "# CountVectorizer?\n",
    "cvt      =  CountVectorizer(strip_accents='unicode', ngram_range=(2,4), min_df=50, stop_words='english')\n",
    "X_all    =  cvt.fit_transform(tweets_df['TEXT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_all.todense(),\n",
    "             columns=cvt.get_feature_names())\n",
    "word_ct = X_train.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_class = tweets_df['LANG']\n",
    "X_train['classes'] = tw_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tw_cl in tw_class.unique():\n",
    "    print tw_cl \n",
    "    plot = X_train[X_train['classes'] == tw_cl].drop('classes', axis=1).sum(axis=0).sort_values(ascending = False).head(20)\n",
    "    plot = pd.DataFrame(plot)\n",
    "    fig, ax = plt.subplots(figsize=(11,6))\n",
    "    g = sns.barplot(plot.index, plot[0])\n",
    "    g.set_xticklabels(g.get_xticklabels(), rotation=45)\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.E) (Optional) Try expanding the list of stop words.\n",
    "There are definitely some non-words, such as web URLs, that could be removed to help us improve the score. Identify word/tokens that don't add much value to either class. **You should also look at n-grams per language to fine-tune your preprocessing. This has the greatest potential to improve your results without tuning any model parameters.**\n",
    "\n",
    "Using `nltk.corpus`, we can get a baseline list of stop words. Try to expand it and pass it to our vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "stop.append('http')\n",
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvt      =  CountVectorizer(strip_accents='unicode', ngram_range=(2,2), stop_words=stop)\n",
    "X_all    =  cvt.fit_transform(tweets_df['TEXT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Set up a train/test split of your data using any method you wish.\n",
    "Try 70/30 to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_all.todense(),\n",
    "             columns=cvt.get_feature_names())\n",
    "tw_class = tweets_df['LANG']\n",
    "X_train['classes'] = tw_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['classes'] = X_train['classes'].astype('category')\n",
    "X_train['classes_cat'] = X_train['classes'].cat.codes\n",
    "X_train['classes_cat'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "tweets_df['LANG'] = tweets_df['LANG'].astype('category')\n",
    "tweets_df['LANG_cat'] = tweets_df['LANG'].cat.codes\n",
    "X_train, X_test, y_train, y_test = train_test_split(tweets_df['TEXT'] , tweets_df['LANG_cat'], stratify=y, test_size=0.2, random_state=42) #can add in stratify=y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Set up a pipeline to vectorize and use the MultinomialNB classifier.\n",
    "Use `lowercase`, `strip_accents`, `Pipeline`, and (optionally) your updated `stop_words`. Fit your comment data using the \"insult\" feature as your response.\n",
    "\n",
    "Fit your training data set to your pipeline, then score it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "stop.append('http')\n",
    "stop.append('https')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's the code — you can adapt it from here on out.\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(strip_accents='unicode', stop_words=stop, ngram_range=(1,2), min_df=1)),\n",
    "#     ('tfidf', TfidfTransformer()),\n",
    "    ('cls', MultinomialNB())\n",
    "#     ('logit', LogisticRegression())\n",
    "]) \n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Don't forget to score.\n",
    "pred = pipeline.predict(X_test)\n",
    "pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.A) Swap out MultinomialNB with BernoulliNB in the pipeline.\n",
    "How do they compare? Do you have a guess as to why BernoulliNB is so poor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's the code — you can adapt it from here on out.\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(strip_accents='unicode', stop_words=stop, ngram_range=(1,2), min_df=1)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('cls', BernoulliNB())\n",
    "#     ('logit', LogisticRegression())\n",
    "]) \n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Don't forget to score.\n",
    "pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.B) Try logistic regression and random forests in the pipeline.\n",
    "How do they compare? Recall that logistic regression is discriminative, whereas Naive Bayes is generative. Logistic regression uses optimization to fit a formula that discriminates between the classes, while Naive Bayes essentially just computes aggregate statistics. So, logistic regression should have a longer training time than Naive Bayes — but does it here? (See `%time`.)\n",
    "\n",
    "**Note**: Logistic regression and random forests both allow you to see feature importance/coefficients. In this case, these coefficients will inform you how strongly each word indicates a language. Optionally, see if you can sort these coefficients by their values to get the strongest and weakest indicator words for languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's the code — you can adapt it from here on out.\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(strip_accents='unicode', stop_words=stop, ngram_range=(1,2), min_df=1)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "#     ('cls', MultinomialNB())\n",
    "    ('logit', LogisticRegression())\n",
    "]) \n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Don't forget to score.\n",
    "pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer(strip_accents='unicode', stop_words=stop, ngram_range=(1,2), min_df=1)\n",
    "X_2 = pd.DataFrame(cvec.fit_transform(X_train).todense(),\n",
    "             columns=cvec.get_feature_names())\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs_vars = pd.DataFrame({\n",
    "        'coef':lr.coef_[0],\n",
    "        'variable':X_2.columns,\n",
    "        'abscoef':np.abs(lr.coef_[0])\n",
    "    })\n",
    "coefs_vars.sort_values('abscoef', ascending=False, inplace=True)\n",
    "coefs_vars.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Here's the code — you can adapt it from here on out.\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(strip_accents='unicode', stop_words=stop, ngram_range=(1,2), min_df=1)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "#     ('cls', MultinomialNB())\n",
    "#     ('logit', LogisticRegression())\n",
    "    ('rf', RandomForestClassifier())\n",
    "]) \n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Don't forget to score.\n",
    "pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer(strip_accents='unicode', stop_words=stop, ngram_range=(1,2), min_df=1)\n",
    "X_2 = pd.DataFrame(cvec.fit_transform(X_train).todense(),\n",
    "             columns=cvec.get_feature_names())\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = rf.feature_importances_\n",
    "std = np.std([rf.feature_importances_ for tree in rf.estimators_],\n",
    "             axis=0)\n",
    "fea_imp = pd.DataFrame(zip(importances,std), X_2.columns.values, columns = ['importances', 'std'])\n",
    "fea_imp = fea_imp.sort_values('importances', ascending=False).reset_index()\n",
    "sns.barplot('importances', 'index', data=fea_imp[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.C) Also, try tweaking the parameters of `CountVectorizer` and `TfidfTranformer`.\n",
    "\n",
    "Remove TF-IDF. Is this good or bad?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Check your score.\n",
    "For which languages does your model work best? Run a classification report for all languages. [Plot the area under curve/ROC](../../week-04/2.3-evaluating_model_fit/code/AUC-ROC-codealong.ipynb) for particular languages (versus all others) and compare them — do they indicate that some languages perform better? Does our model perform worse while guessing on some languages versus others? Additionally, [review the classification reporting metrics](../../week-04/4.3-advanced-model_evaluation/code/starter-code/week4-4.1-classification-report.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the code to display the classification report.\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = tweets_df['LANG'].astype('object').unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix and f1-score\n",
    "conmat = np.array(confusion_matrix(y_test, pred))\n",
    "confusion = pd.DataFrame(conmat, index=index, columns='pred_'+index)\n",
    "print (confusion)\n",
    "print (classification_report(y_test, pred, target_names=index))\n",
    "metrics.f1_score(y_test, pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revisiting: ROC/AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_roc(y, probs):\n",
    "    \n",
    "    mean_tpr = 0.0\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    all_tpr = []\n",
    "\n",
    "    for i, (train, test) in enumerate(cv):\n",
    "        probas_ = classifier.fit(X[train], y[train]).predict_proba(X[test])\n",
    "        # Compute the ROC curve and area under the curve.\n",
    "        fpr, tpr, thresholds = roc_curve(y[test], probas_[:, 1])\n",
    "        mean_tpr += interp(mean_fpr, fpr, tpr)\n",
    "        mean_tpr[0] = 0.0\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, lw=1, label='ROC fold %d (area = %0.2f)' % (i, roc_auc))\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], '--', color=(0.6, 0.6, 0.6), label='Luck')\n",
    "\n",
    "    mean_tpr /= len(cv)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    plt.plot(mean_fpr, mean_tpr, 'k--',\n",
    "             label='Mean ROC (area = %0.2f)' % mean_auc, lw=2)\n",
    "\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "def plot_roc(y, probs, threshmarkers=None):\n",
    "    fpr, tpr, thresh = roc_curve(y, probs)\n",
    "\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.plot(fpr, tpr, lw=2)\n",
    "   \n",
    "    plt.xlabel(\"False Positive Rate\\n(1 - Specificity)\")\n",
    "    plt.ylabel(\"True Positive Rate\\n(Sensitivity)\")\n",
    "    plt.xlim([-0.025, 1.025])\n",
    "    plt.ylim([-0.025, 1.025])\n",
    "    plt.xticks(np.linspace(0, 1, 21), rotation=45)\n",
    "    plt.yticks(np.linspace(0, 1, 21))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.7*len(tweets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df\n",
    "df_train = tweets_df.sample(6586).copy()\n",
    "df_test = tweets_df[~tweets_df.index.isin(tweets_df.index)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(strip_accents='unicode', stop_words=stop, ngram_range=(1,2), min_df=1)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('cls', MultinomialNB())\n",
    "#     ('logit', LogisticRegression())\n",
    "#     ('rf', RandomForestClassifier())\n",
    "]) \n",
    "pipeline.fit(df_train.drop('LANG',axis=1), df_train['LANG'])\n",
    "# Don't forget to score.\n",
    "pipeline.score(X_test, y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using your pipeline, predict the probabilities of each language.\n",
    "# Then, call plot_roc.\n",
    "\n",
    "## Your code to predict the probabilities of each class:\n",
    "\n",
    "# Example of testing a particular language:\n",
    "plot_roc(df_test['LANG'].apply(lambda x: x == \"en\"), predicted_proba[:, list(pipeline.classes_).index(\"en\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Check out your baseline.\n",
    "\n",
    "What is the chance that you'll randomly guess correctly without any modeling? Assume your input phrase's language has the same chance of appearing as the languages in your training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) What is your model not getting right?\n",
    "\n",
    "Check out the incorrectly classified tweets. Are there any noticeable patterns? Can you explain why many of these are incorrectly classified given what you know about how Naive Bayes works? Pay particular attention to the recall metric.  What could be done in the preprocessing steps to improve accuracy?  \n",
    "\n",
    "- Try to improve your **preprocessing first**.\n",
    "- Then, try to tweak your **parameters to your model(s)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Practice\n",
    "\n",
    "There are two additional data sets in the directory that you can use for more practice:\n",
    "\n",
    "- **/datasets/tweets_sentiment.csv**: Sentiment analysis.\n",
    "\n",
    "- **/datasets/insults_train.csv**: [Kaggle data set](https://www.kaggle.com/c/detecting-insults-in-social-commentary). _Warning:_ This content is fairly provocative and contains offensive and insensitive words. However, this type of problem is common in the continuum of comment threads throughout the web.\n",
    "\n",
    "    - Check out [this blog post](http://webmining.olariu.org/my-first-kaggle-competition-and-how-i-ranked/) by a guy who used support vector machines, a \"neural network,\" and a ton of cleaning to place third in a Kaggle competition using this same data set. Additionally, see [this post](http://peekaboo-vision.blogspot.de/2012/09/recap-of-my-first-kaggle-competition.html) — he took sixth place and found that the best model was a simple logistic regression.\n",
    "\n",
    "#### Where to next?\n",
    "\n",
    "If you're interested in this type of problem, a great area to read up on is sentiment analysis. This [Kaggle data set](https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data) offers an excellent opportunity for more practice.  The following white papers are also great for further exploration in this topic:\n",
    "\n",
    "- [Fast and accurate sentiment classification using an enhanced Naive Bayes model](http://arxiv.org/pdf/1305.6143.pdf)— *a great overview!*\n",
    "- [Sarcasm detection](http://www.aclweb.org/anthology/P15-2124).\n",
    "- [Making computers laugh: Investigations in automatic humor recognition](http://www.aclweb.org/anthology/H05-1067).\n",
    "- [Modeling sarcasm in Twitter, a novel approach](http://www.aclweb.org/anthology/W14-2609).\n",
    "- [Narcissism and lie detection](https://deepblue.lib.umich.edu/bitstream/handle/2027.42/107345/zarins.finalthesis.pdf?sequence=1) — *this study's metrics are interesting.*"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
