{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Natural Language Processing Lab\n",
    "\n",
    "_Authors: Dave Yerrington (SF)_\n",
    "\n",
    "---\n",
    "\n",
    "In this lab, we'll explore scikit-learn and NLTK's capabilities for processing text even further. We'll use the 20 newsgroups data set, which is provided by scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard data science imports:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the scikit-learn data set:\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Use the `fetch_20newsgroups` function to download a training and testing set.\n",
    "\n",
    "Look up the function documentation for how to grab the data.\n",
    "\n",
    "You should pull these categories:\n",
    "- `alt.atheism`\n",
    "- `talk.religion.misc`\n",
    "- `comp.graphics`\n",
    "- `sci.space`\n",
    "\n",
    "Additionally, remove the headers, footers, and quotes using the function's `remove` keyword argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    }
   ],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'talk.religion.misc','comp.graphics','sci.space']\n",
    "newsgroups_train2 = fetch_20newsgroups(subset='train',categories=categories, remove=('headers', 'footers', 'quotes'))\n",
    "newsgroups_test2 = fetch_20newsgroups(subset='test',categories=categories, remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2034L,)\n",
      "[u\"Hi,\\n\\nI've noticed that if you only save a model (with all your mapping planes\\npositioned carefully) to a .3DS file that when you reload it after restarting\\n3DS, they are given a default position and orientation.  But if you save\\nto a .PRJ file their positions/orientation are preserved.  Does anyone\\nknow why this information is not stored in the .3DS file?  Nothing is\\nexplicitly said in the manual about saving texture rules in the .PRJ file. \\nI'd like to be able to read the texture rule information, does anyone have \\nthe format for the .PRJ file?\\n\\nIs the .CEL file format available from somewhere?\\n\\nRych\"]\n"
     ]
    }
   ],
   "source": [
    "print newsgroups_train2.target.shape\n",
    "print newsgroups_train2.data[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Inspect the data.\n",
    "\n",
    "We've downloaded a few `newsgroups` categories and removed their headers, footers, and quotes.\n",
    "\n",
    "Because this is a scikit-learn data set, it comes with pre-split training and testing sets (note: we were able to call \"train\" and \"test\" in subset).\n",
    "\n",
    "Let's inspect them.\n",
    "\n",
    "1) What data type is `data_train`?\n",
    "- Is it a list? A dictionary? What else?\n",
    "- How many data points does it contain?\n",
    "- Inspect the first data point. What does it look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n",
      "2034\n",
      "Hi,\n",
      "\n",
      "I've noticed that if you only save a model (with all your mapping planes\n",
      "positioned carefully) to a .3DS file that when you reload it after restarting\n",
      "3DS, they are given a default position and orientation.  But if you save\n",
      "to a .PRJ file their positions/orientation are preserved.  Does anyone\n",
      "know why this information is not stored in the .3DS file?  Nothing is\n",
      "explicitly said in the manual about saving texture rules in the .PRJ file. \n",
      "I'd like to be able to read the texture rule information, does anyone have \n",
      "the format for the .PRJ file?\n",
      "\n",
      "Is the .CEL file format available from somewhere?\n",
      "\n",
      "Rych\n"
     ]
    }
   ],
   "source": [
    "print type(newsgroups_train2.data)\n",
    "print len(newsgroups_train2.data)\n",
    "print newsgroups_train2.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n",
      "1353\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print type(newsgroups_test2.target)\n",
    "print len(newsgroups_test2.target)\n",
    "print newsgroups_test2.target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 0, 3], dtype=int64)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(newsgroups_test2.target)[0].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Create a bag-of-words model.\n",
    "\n",
    "Let's train a model using a simple count vectorizer.\n",
    "\n",
    "1) Initialize a standard CountVectorizer and fit the training data.\n",
    "- How big is the feature dictionary?\n",
    "- Eliminate English stop words.\n",
    "- Is the dictionary smaller?\n",
    "- Transform the training data using the trained vectorizer.\n",
    "- Evaluate the performance of a logistic regression on the features extracted by the CountVectorizer.\n",
    "    - You will have to transform the `test_set`, too. Be careful to use the trained vectorizer without refitting it.\n",
    "\n",
    "**Bonus**\n",
    "- Try a couple of modifications:\n",
    "    - Restrict the `max_features`.\n",
    "    - Change the `max_df` and `min_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26576\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#max_features: build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.\n",
    "#max_df: if no stop words will be used. max_df can be set to a value in the range [0.7, 1.0) \n",
    "#to automatically detect and filter stop words based on intra corpus document frequency of terms.\n",
    "#min_df: When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. \n",
    "#If float, the parameter represents a proportion of documents, integer absolute counts.\n",
    "cvec = CountVectorizer(stop_words='english')\n",
    "# cvec = CountVectorizer(max_df=0.999, min_df=0.001)\n",
    "cvec.fit(newsgroups_train2.data)\n",
    "print len(cvec.vocabulary_)#26576 with stop_words, 26879 without stop words, 12442 words with max_df=0.999, min_df=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0000</th>\n",
       "      <th>00000</th>\n",
       "      <th>000000</th>\n",
       "      <th>000005102000</th>\n",
       "      <th>000062david42</th>\n",
       "      <th>0001</th>\n",
       "      <th>000100255pixel</th>\n",
       "      <th>00041032</th>\n",
       "      <th>...</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zurvanism</th>\n",
       "      <th>zus</th>\n",
       "      <th>zvi</th>\n",
       "      <th>zwaartepunten</th>\n",
       "      <th>zwak</th>\n",
       "      <th>zwakke</th>\n",
       "      <th>zware</th>\n",
       "      <th>zwarte</th>\n",
       "      <th>zyxel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 26576 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  000  0000  00000  000000  000005102000  000062david42  0001  \\\n",
       "0   0    0     0      0       0             0              0     0   \n",
       "1   0    0     0      0       0             0              0     0   \n",
       "2   0    0     0      0       0             0              0     0   \n",
       "3   0    0     0      0       0             0              0     0   \n",
       "4   0    0     0      0       0             0              0     0   \n",
       "\n",
       "   000100255pixel  00041032  ...    zurich  zurvanism  zus  zvi  \\\n",
       "0               0         0  ...         0          0    0    0   \n",
       "1               0         0  ...         0          0    0    0   \n",
       "2               0         0  ...         0          0    0    0   \n",
       "3               0         0  ...         0          0    0    0   \n",
       "4               0         0  ...         0          0    0    0   \n",
       "\n",
       "   zwaartepunten  zwak  zwakke  zware  zwarte  zyxel  \n",
       "0              0     0       0      0       0      0  \n",
       "1              0     0       0      0       0      0  \n",
       "2              0     0       0      0       0      0  \n",
       "3              0     0       0      0       0      0  \n",
       "4              0     0       0      0       0      0  \n",
       "\n",
       "[5 rows x 26576 columns]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train  = pd.DataFrame(cvec.transform(newsgroups_train2.data).todense(),\n",
    "             columns=cvec.get_feature_names())\n",
    "X_train.head()\n",
    "# X_train.transpose().sort_values(0, ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorize test set as well\n",
    "X_test = pd.DataFrame(cvec.transform(newsgroups_test2.data).todense(),\n",
    "             columns=cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "clf = MultinomialNB(alpha=0.1)\n",
    "clf.fit(X_train, newsgroups_train2.target)\n",
    "pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    394\n",
       "1    389\n",
       "0    319\n",
       "3    251\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(newsgroups_test2.target)[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      predicted_0  predicted_1  predicted_2  predicted_3\n",
      "alt.atheism_0                 227            4           28           60\n",
      "talk.religion.misc_1           11          351           24            3\n",
      "comp.graphics_2                19           21          343           11\n",
      "sci.space_3                    82            7           21          141\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.67      0.71      0.69       319\n",
      "          1       0.92      0.90      0.91       389\n",
      "          2       0.82      0.87      0.85       394\n",
      "          3       0.66      0.56      0.61       251\n",
      "\n",
      "avg / total       0.78      0.78      0.78      1353\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7628399561432191"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "conmat = np.array(confusion_matrix(newsgroups_test2.target, pred))\n",
    "confusion = pd.DataFrame(conmat, index=['alt.atheism_0', 'talk.religion.misc_1','comp.graphics_2','sci.space_3'],\n",
    "                         columns=['predicted_0','predicted_1','predicted_2','predicted_3'])\n",
    "print confusion\n",
    "print classification_report(newsgroups_test2.target, pred)\n",
    "metrics.f1_score(newsgroups_test2.target, pred, average='macro')#0.76 with stop_words, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "ath = fetch_20newsgroups(subset='test', categories=['alt.atheism'], remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "319"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ath.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridsearch through pipeline to find best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n",
      "Loading 4 newsgroups dataset for categories:\n",
      "['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
      "2034 documents\n",
      "4 categories\n",
      "\n",
      "Performing grid search...\n",
      "pipeline: ['vect', 'clf']\n",
      "parameters:\n",
      "{'clf__alpha': (0.05, 0.1, 0.15),\n",
      " 'vect__max_df': (0.5, 0.75, 1.0),\n",
      " 'vect__max_features': (None, 1000, 20000, 30000),\n",
      " 'vect__min_df': (0.1, 0.01)}\n",
      "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   26.5s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 216 out of 216 | elapsed:  2.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.760\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.15\n",
      "\tvect__max_df: 0.5\n",
      "\tvect__max_features: None\n",
      "\tvect__min_df: 0.01\n"
     ]
    }
   ],
   "source": [
    "# Author: Olivier Grisel <olivier.grisel@ensta.org>\n",
    "#         Peter Prettenhofer <peter.prettenhofer@gmail.com>\n",
    "#         Mathieu Blondel <mathieu@mblondel.org>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "# from sklearn.feature_extraction.text import TfidfTransformer\n",
    "# from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "# Display progress logs on stdout\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s %(message)s')\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Load some categories from the training set\n",
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "# Uncomment the following to do the analysis on all the categories\n",
    "#categories = None\n",
    "\n",
    "print(\"Loading 4 newsgroups dataset for categories:\")\n",
    "print(categories)\n",
    "\n",
    "train = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
    "print(\"%d documents\" % len(train.filenames))\n",
    "print(\"%d categories\" % len(train.target_names))\n",
    "print()\n",
    "\n",
    "# #############################################################################\n",
    "# Define a pipeline combining a text feature extractor with a simple\n",
    "# classifier\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(stop_words='english')),\n",
    "    #('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "# uncommenting more parameters will give better exploring power but will\n",
    "# increase processing time in a combinatorial way\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    'vect__min_df': (0.1, 0.01),\n",
    "    'vect__max_features': (None, 1000, 20000, 30000),\n",
    "#     'vect__ngram_range': ((1, 1), (1, 2), (1, 3)),  # unigrams or bigrams\n",
    "    #'tfidf__use_idf': (True, False),\n",
    "    #'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__alpha': (0.05, 0.1, 0.15),\n",
    "    #'clf__penalty': ('l2', 'elasticnet'),\n",
    "    #'clf__n_iter': (10, 50, 80),\n",
    "}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # multiprocessing requires the fork to happen in a __main__ protected\n",
    "    # block\n",
    "\n",
    "    # find the best parameters for both the feature extraction and the\n",
    "    # classifier\n",
    "    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
    "\n",
    "    print(\"Performing grid search...\")\n",
    "    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "    print(\"parameters:\")\n",
    "    pprint(parameters)\n",
    "#     t0 = time()\n",
    "    grid_search.fit(train.data, train.target)\n",
    "#     print(\"done in %0.3fs\" % (time() - t0))\n",
    "#     print()\n",
    "\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cvec = CountVectorizer(stop_words='english',max_df=0.5)\n",
    "# cvec.fit(data.data)\n",
    "# X_train = cvec.transform(data.data)\n",
    "# clf = MultinomialNB(alpha=0.15)\n",
    "# clf.fit(X_train, data.target)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "news_class = Pipeline([('vect', CountVectorizer(stop_words='english',max_df=0.9)),\n",
    "#                        [('vect', TfidfVectorizer(stop_words='english',max_df=0.9)),\n",
    "                       ('clf', MultinomialNB(alpha=0.15))\n",
    "#                        ('logit', LogisticRegression())])\n",
    "news_class_fit = news_class.fit(data.data, data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = fetch_20newsgroups(subset='test',categories=categories, remove=('headers', 'footers', 'quotes'))\n",
    "news_class_pred = news_class_fit.predict(test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      predicted_0  predicted_1  predicted_2  predicted_3\n",
      "alt.atheism_0                 187           16           46           70\n",
      "talk.religion.misc_1           13          345           28            3\n",
      "comp.graphics_2                22           23          333           16\n",
      "sci.space_3                    67           14           27          143\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.65      0.59      0.62       319\n",
      "          1       0.87      0.89      0.88       389\n",
      "          2       0.77      0.85      0.80       394\n",
      "          3       0.62      0.57      0.59       251\n",
      "\n",
      "avg / total       0.74      0.75      0.74      1353\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.72208976281306"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conmat = np.array(confusion_matrix(test.target, news_class_pred))\n",
    "confusion = pd.DataFrame(conmat, index=['alt.atheism_0', 'talk.religion.misc_1','comp.graphics_2','sci.space_3'],\n",
    "                         columns=['predicted_0','predicted_1','predicted_2','predicted_3'])\n",
    "print (confusion)\n",
    "print (classification_report(test.target, news_class_pred))\n",
    "metrics.f1_score(test.target, news_class_pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Test Out Hashing and TF-IDF.\n",
    "\n",
    "Let's see if hashing or TF-IDF improves our accuracy.\n",
    "\n",
    "1) Initialize a HashingVectorizer and repeat the test with no restriction on the number of features.\n",
    "- Does the score improve with respect to the CountVectorizer?\n",
    "- Print out the number of features for this model.\n",
    "- Initialize a TF-IDF vectorizer and repeat the analysis above.\n",
    "- Print out the number of features for this model.\n",
    "\n",
    "**Bonus**\n",
    "- Change the parameters of either (or both) models to improve your score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline\n",
    "pclass = Pipeline([('vect', TfidfVectorizer(stop_words='english',max_df=0.9)),\n",
    "                       ('clf', MultinomialNB(alpha=0.15))])\n",
    "#                        ('logit', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit and predict\n",
    "class_fit = pclass.fit(data.data, data.target)\n",
    "class_pred = class_fit.predict(test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      predicted_0  predicted_1  predicted_2  predicted_3\n",
      "alt.atheism_0                 228           10           40           41\n",
      "talk.religion.misc_1            7          358           23            1\n",
      "comp.graphics_2                19           18          355            2\n",
      "sci.space_3                    89           11           24          127\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.66      0.71      0.69       319\n",
      "          1       0.90      0.92      0.91       389\n",
      "          2       0.80      0.90      0.85       394\n",
      "          3       0.74      0.51      0.60       251\n",
      "\n",
      "avg / total       0.79      0.79      0.78      1353\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7627353148351768"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#confusion matrix and f1-score\n",
    "conmat = np.array(confusion_matrix(test.target, class_pred))\n",
    "confusion = pd.DataFrame(conmat, index=['alt.atheism_0', 'talk.religion.misc_1','comp.graphics_2','sci.space_3'],         columns=['predicted_0','predicted_1','predicted_2','predicted_3'])\n",
    "print (confusion)\n",
    "print (classification_report(test.target, class_pred))\n",
    "metrics.f1_score(test.target, class_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
